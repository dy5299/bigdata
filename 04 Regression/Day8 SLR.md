# 단순선형회귀

## statsmodels의 summary 설명

종속변수: y

결정계수: R-squared

사용한 기법: OLS

const, x1: 절편과 계수

P-value: 유의확률

P>|t|: 유의 확률이 유의계수보다 크면, 이 회귀 모형에 찾아진 분포가 맞지 않다.

## 결정 계수

상관 계수: 두 변수들 간에 얼마나 밀접한가.

->파생-> 결정계수. 모든 계수들의 상관 계수.

## Q. b

절편값은 그 자체로 종속적으로 결정된 것. 절편은 궁금하지 않아. 단순히 수학적 의미로, 종속 변수가 모두 0일 때 나오는 산술적인 값이다.

## Q. sklearn과 statsmodels 결정계수가 다른데

데이터가 달라.

# 다중선형회귀

## 다중 공선성 문제

원래는 종속변수였어야 하는게 독립변수로 되어있다면, 당연히 상관관계가 높게 나온다.



상관계수

데이터 분야에 따라 다르지만.

0.5: 어느 정도 있음



수정된 결정 계수

독립변수의 개수가 많을수록 종속변수에 대한 설명력은 증가한다.

당연히 설명력이(결정계수가) 높아짐.

요인 자체가 많다는 것으로 설명력이 높다는게 맞는가?

수정 결정 계수(adjusted R^2): 결정계수가 과대평가된 것 같아서 다중회귀에서 이를 보정한 값

이는 항상 결정계수보다 작거나 같은 값. 아주 약간 튜닝.





독립변수 탈락은 다중공선성을 확인한 뒤.

평가 시 수정된 결정계수 사용.



# 경사 하강법

day9 (200716)

데이터에 대한 고찰, 의미를 숙고한 다음

의미가 있는 데이터일 때 분석



경사하강법은 회귀분석만의 이론은 아니고,

기계분석 전반에 쓰이는 개념.

단점:  전역 최적값(global minimum)을 찾기 전에 경사가 0인 지점을 만날 수도 있다.



## 경사 하강법의 유형

- 배치 경사 하강법 (batch gradient descent)
  - 1개의 배치에 전체 학습 데이터 모두 들어간다.
  - 일관된 방향을 향해 지속적으로 접근
  - 데이터가 많으면 수행 시간이 오래 걸린다.
  - 데이터가 적을 때 사용. (record 개수가 1000개 정도 ~3000개). 10000개 넘어가면 성능이 뚝 떨어진다.
- 확률적 경사 하강법 (SGD; stochastic gradient descent)
  - 1개의 배치에 임의의 학습 데이터 1개만 들어간다.
  - 데이터가 많아도 수행 시간이 빠르다.
  - 대신 반복을 많이 해야 한다.
  - 무작위 선택으로 인해 노이즈가 많을 수 있고, 데이터 편중이 있을 수 있다.
- 미니배치 경사 하강법 (mini-batch gradient descent)
  - 1개의 배치에 임의의 학습 데이터 여러 개가 들어간다.
  - 앞의 두 가지 방법의 절충안.
  - 전체 배치보다는 효율적이고, 확률 경사 하강법보다는 노이즈가 적다.
  - local optimum에 빠지면 global optimum을 찾지 못하는 문제가 있다.
  - GPU 사용하는 신경망에서 많이 사용한다.

사실 많이 사용되는 것은 SGD이다.

- 워낙 빠르기 떄문
- Scikit learn에서 SGD만 구현되어 있는 현실적 문제로 인해.

### 용어

- Batch: 1회의 경사 업데이트에 사용되는 데이터 집합
  - batch size: 이때 사용되는 데이터 집합의 개수
- Epoch: iteration
  - 전체 데이터들을 한 번 사용한 횟수, 즉 학습의 반복 횟수

learning_rate

eta0: 첫 학습률

학습률을 바꿔가며 하는게 좋겠지.

learning_rate: invscaling.

eta0를 바꾸면 돼